{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VaradBelwalkar/NLP_Seq2Seq/blob/main/ENG_FRE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## english to French Seq2Seq model"
      ],
      "metadata": {
        "id": "a6YDaLolkco4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "FQCmps7kEsZt"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense, Input, Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from collections import Counter\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile\n",
        "import sys\n",
        "import urllib.request\n",
        "from gensim.models import KeyedVectors\n",
        "from keras.utils import plot_model\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "evak6GnuEsZw"
      },
      "outputs": [],
      "source": [
        "from keras.activations import softmax\n",
        "from keras.layers import Dense, Activation, RepeatVector, Permute\n",
        "from keras.layers import Input, Embedding, Multiply, Concatenate, Lambda\n",
        "from keras.layers import TimeDistributed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JYHPE4NEsZx"
      },
      "source": [
        "#hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pD7U6tK4EsZz"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 100\n",
        "HIDDEN_UNITS = 256\n",
        "NUM_SAMPLES = 10000\n",
        "MAX_VOCAB_SIZE = 10000\n",
        "EMBEDDING_SIZE = 100\n",
        "DATA = 'fra.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "P1Jj6A5PEsZ1"
      },
      "outputs": [],
      "source": [
        "tar_count = Counter()\n",
        "\n",
        "GLOVE_MODEL = \"glove.6B.100d.txt\"\n",
        "WEIGHT_FILE_PATH = 'eng-to-fr-glove-weights.h5'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the GloVe embeddings zip"
      ],
      "metadata": {
        "id": "RCjsSYtwnXob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VXLT49jF91Y",
        "outputId": "f6850a9d-9f90-4e18-aa5c-400d72b7c866"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-08 04:25:35--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-06-08 04:25:36--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-06-08 04:25:36--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2024-06-08 04:28:15 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting the zip to get required embeddings"
      ],
      "metadata": {
        "id": "15eB_nKbneyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWV_9c86HyhT",
        "outputId": "895ef3c1-60b4-48ee-85e7-4faf0c3b94c9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample word embeddings"
      ],
      "metadata": {
        "id": "LcQb2_TwnjjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head glove.6B.100d.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWvHayPMISLU",
        "outputId": "e0f231b0-59ca-4b59-e331-978b74e4f09b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n",
            ", -0.10767 0.11053 0.59812 -0.54361 0.67396 0.10663 0.038867 0.35481 0.06351 -0.094189 0.15786 -0.81665 0.14172 0.21939 0.58505 -0.52158 0.22783 -0.16642 -0.68228 0.3587 0.42568 0.19021 0.91963 0.57555 0.46185 0.42363 -0.095399 -0.42749 -0.16567 -0.056842 -0.29595 0.26037 -0.26606 -0.070404 -0.27662 0.15821 0.69825 0.43081 0.27952 -0.45437 -0.33801 -0.58184 0.22364 -0.5778 -0.26862 -0.20425 0.56394 -0.58524 -0.14365 -0.64218 0.0054697 -0.35248 0.16162 1.1796 -0.47674 -2.7553 -0.1321 -0.047729 1.0655 1.1034 -0.2208 0.18669 0.13177 0.15117 0.7131 -0.35215 0.91348 0.61783 0.70992 0.23955 -0.14571 -0.37859 -0.045959 -0.47368 0.2385 0.20536 -0.18996 0.32507 -1.1112 -0.36341 0.98679 -0.084776 -0.54008 0.11726 -1.0194 -0.24424 0.12771 0.013884 0.080374 -0.35414 0.34951 -0.7226 0.37549 0.4441 -0.99059 0.61214 -0.35111 -0.83155 0.45293 0.082577\n",
            ". -0.33979 0.20941 0.46348 -0.64792 -0.38377 0.038034 0.17127 0.15978 0.46619 -0.019169 0.41479 -0.34349 0.26872 0.04464 0.42131 -0.41032 0.15459 0.022239 -0.64653 0.25256 0.043136 -0.19445 0.46516 0.45651 0.68588 0.091295 0.21875 -0.70351 0.16785 -0.35079 -0.12634 0.66384 -0.2582 0.036542 -0.13605 0.40253 0.14289 0.38132 -0.12283 -0.45886 -0.25282 -0.30432 -0.11215 -0.26182 -0.22482 -0.44554 0.2991 -0.85612 -0.14503 -0.49086 0.0082973 -0.17491 0.27524 1.4401 -0.21239 -2.8435 -0.27958 -0.45722 1.6386 0.78808 -0.55262 0.65 0.086426 0.39012 1.0632 -0.35379 0.48328 0.346 0.84174 0.098707 -0.24213 -0.27053 0.045287 -0.40147 0.11395 0.0062226 0.036673 0.018518 -1.0213 -0.20806 0.64072 -0.068763 -0.58635 0.33476 -1.1432 -0.1148 -0.25091 -0.45907 -0.096819 -0.17946 -0.063351 -0.67412 -0.068895 0.53604 -0.87773 0.31802 -0.39242 -0.23394 0.47298 -0.028803\n",
            "of -0.1529 -0.24279 0.89837 0.16996 0.53516 0.48784 -0.58826 -0.17982 -1.3581 0.42541 0.15377 0.24215 0.13474 0.41193 0.67043 -0.56418 0.42985 -0.012183 -0.11677 0.31781 0.054177 -0.054273 0.35516 -0.30241 0.31434 -0.33846 0.71715 -0.26855 -0.15837 -0.47467 0.051581 -0.33252 0.15003 -0.1299 -0.54617 -0.37843 0.64261 0.82187 -0.080006 0.078479 -0.96976 -0.57741 0.56491 -0.39873 -0.057099 0.19743 0.065706 -0.48092 -0.20125 -0.40834 0.39456 -0.02642 -0.11838 1.012 -0.53171 -2.7474 -0.042981 -0.74849 1.7574 0.59085 0.04885 0.78267 0.38497 0.42097 0.67882 0.10337 0.6328 -0.026595 0.58647 -0.44332 0.33057 -0.12022 -0.55645 0.073611 0.20915 0.43395 -0.012761 0.089874 -1.7991 0.084808 0.77112 0.63105 -0.90685 0.60326 -1.7515 0.18596 -0.50687 -0.70203 0.66578 -0.81304 0.18712 -0.018488 -0.26757 0.727 -0.59363 -0.34839 -0.56094 -0.591 1.0039 0.20664\n",
            "to -0.1897 0.050024 0.19084 -0.049184 -0.089737 0.21006 -0.54952 0.098377 -0.20135 0.34241 -0.092677 0.161 -0.13268 -0.2816 0.18737 -0.42959 0.96039 0.13972 -1.0781 0.40518 0.50539 -0.55064 0.4844 0.38044 -0.0029055 -0.34942 -0.099696 -0.78368 1.0363 -0.2314 -0.47121 0.57126 -0.21454 0.35958 -0.48319 1.0875 0.28524 0.12447 -0.039248 -0.076732 -0.76343 -0.32409 -0.5749 -1.0893 -0.41811 0.4512 0.12112 -0.51367 -0.13349 -1.1378 -0.28768 0.16774 0.55804 1.5387 0.018859 -2.9721 -0.24216 -0.92495 2.1992 0.28234 -0.3478 0.51621 -0.43387 0.36852 0.74573 0.072102 0.27931 0.92569 -0.050336 -0.85856 -0.1358 -0.92551 -0.33991 -1.0394 -0.067203 -0.21379 -0.4769 0.21377 -0.84008 0.052536 0.59298 0.29604 -0.67644 0.13916 -1.5504 -0.20765 0.7222 0.52056 -0.076221 -0.15194 -0.13134 0.058617 -0.31869 -0.61419 -0.62393 -0.41548 -0.038175 -0.39804 0.47647 -0.15983\n",
            "and -0.071953 0.23127 0.023731 -0.50638 0.33923 0.1959 -0.32943 0.18364 -0.18057 0.28963 0.20448 -0.5496 0.27399 0.58327 0.20468 -0.49228 0.19974 -0.070237 -0.88049 0.29485 0.14071 -0.1009 0.99449 0.36973 0.44554 0.28998 -0.1376 -0.56365 -0.029365 -0.4122 -0.25269 0.63181 -0.44767 0.24363 -0.10813 0.25164 0.46967 0.3755 -0.23613 -0.14129 -0.44537 -0.65737 -0.042421 -0.28636 -0.28811 0.063766 0.20281 -0.53542 0.41307 -0.59722 -0.38614 0.19389 -0.17809 1.6618 -0.011819 -2.3737 0.058427 -0.2698 1.2823 0.81925 -0.22322 0.72932 -0.053211 0.43507 0.85011 -0.42935 0.92664 0.39051 1.0585 -0.24561 -0.18265 -0.5328 0.059518 -0.66019 0.18991 0.28836 -0.2434 0.52784 -0.65762 -0.14081 1.0491 0.5134 -0.23816 0.69895 -1.4813 -0.2487 -0.17936 -0.059137 -0.08056 -0.48782 0.014487 -0.6259 -0.32367 0.41862 -1.0807 0.46742 -0.49931 -0.71895 0.86894 0.19539\n",
            "in 0.085703 -0.22201 0.16569 0.13373 0.38239 0.35401 0.01287 0.22461 -0.43817 0.50164 -0.35874 -0.34983 0.055156 0.69648 -0.17958 0.067926 0.39101 0.16039 -0.26635 -0.21138 0.53698 0.49379 0.9366 0.66902 0.21793 -0.46642 0.22383 -0.36204 -0.17656 0.1748 -0.20367 0.13931 0.019832 -0.10413 -0.20244 0.55003 -0.1546 0.98655 -0.26863 -0.2909 -0.32866 -0.34188 -0.16943 -0.42001 -0.046727 -0.16327 0.70824 -0.74911 -0.091559 -0.96178 -0.19747 0.10282 0.55221 1.3816 -0.65636 -3.2502 -0.31556 -1.2055 1.7709 0.4026 -0.79827 1.1597 -0.33042 0.31382 0.77386 0.22595 0.52471 -0.034053 0.32048 0.079948 0.17752 -0.49426 -0.70045 -0.44569 0.17244 0.20278 0.023292 -0.20677 -1.0158 0.18325 0.56752 0.31821 -0.65011 0.68277 -0.86585 -0.059392 -0.29264 -0.55668 -0.34705 -0.32895 0.40215 -0.12746 -0.20228 0.87368 -0.545 0.79205 -0.20695 -0.074273 0.75808 -0.34243\n",
            "a -0.27086 0.044006 -0.02026 -0.17395 0.6444 0.71213 0.3551 0.47138 -0.29637 0.54427 -0.72294 -0.0047612 0.040611 0.043236 0.29729 0.10725 0.40156 -0.53662 0.033382 0.067396 0.64556 -0.085523 0.14103 0.094539 0.74947 -0.194 -0.68739 -0.41741 -0.22807 0.12 -0.48999 0.80945 0.045138 -0.11898 0.20161 0.39276 -0.20121 0.31354 0.75304 0.25907 -0.11566 -0.029319 0.93499 -0.36067 0.5242 0.23706 0.52715 0.22869 -0.51958 -0.79349 -0.20368 -0.50187 0.18748 0.94282 -0.44834 -3.6792 0.044183 -0.26751 2.1997 0.241 -0.033425 0.69553 -0.64472 -0.0072277 0.89575 0.20015 0.46493 0.61933 -0.1066 0.08691 -0.4623 0.18262 -0.15849 0.020791 0.19373 0.063426 -0.31673 -0.48177 -1.3848 0.13669 0.96859 0.049965 -0.2738 -0.035686 -1.0577 -0.24467 0.90366 -0.12442 0.080776 -0.83401 0.57201 0.088945 -0.42532 -0.018253 -0.079995 -0.28581 -0.01089 -0.4923 0.63687 0.23642\n",
            "\" -0.30457 -0.23645 0.17576 -0.72854 -0.28343 -0.2564 0.26587 0.025309 -0.074775 -0.3766 -0.057774 0.12159 0.34384 0.41928 -0.23236 -0.31547 0.60939 0.25117 -0.68667 0.70873 1.2162 -0.1824 -0.48442 -0.33445 0.30343 1.086 0.49992 -0.20198 0.27959 0.68352 -0.33566 -0.12405 0.059656 0.33617 0.37501 0.56552 0.44867 0.11284 -0.16196 -0.94346 -0.67961 0.18581 0.060653 0.43776 0.13834 -0.48207 -0.56141 -0.25422 -0.52445 0.097003 -0.48925 0.19077 0.21481 1.4969 -0.86665 -3.2846 0.56854 0.41971 1.2294 0.78522 -0.29369 0.63803 -1.5926 -0.20437 1.5306 0.13548 0.50722 0.18742 0.48552 -0.28995 0.19573 0.0046515 0.092879 -0.42444 0.64987 0.52839 0.077908 0.8263 -1.2208 -0.34955 0.49855 -0.64155 -0.72308 0.26566 -1.3643 -0.46364 -0.52048 -1.0525 0.22895 -0.3456 -0.658 -0.16735 0.35158 0.74337 0.26074 0.061104 -0.39079 -0.84557 -0.035432 0.17036\n",
            "'s 0.58854 -0.2025 0.73479 -0.68338 -0.19675 -0.1802 -0.39177 0.34172 -0.60561 0.63816 -0.26695 0.36486 -0.40379 -0.1134 -0.58718 0.2838 0.8025 -0.35303 0.30083 0.078935 0.44416 -0.45906 0.79294 0.50365 0.32805 0.28027 -0.4933 -0.38482 -0.039284 -0.2483 -0.1988 1.1469 0.13228 0.91691 -0.36739 0.89425 0.5426 0.61738 -0.62205 -0.31132 -0.50933 0.23335 1.0826 -0.044637 -0.12767 0.27628 -0.032617 -0.27397 0.77764 -0.50861 0.038307 -0.33679 0.42344 1.2271 -0.53826 -3.2411 0.42626 0.025189 1.3948 0.65085 0.03325 0.37141 0.4044 0.35558 0.98265 -0.61724 0.53901 0.76219 0.30689 0.33065 0.30956 -0.15161 -0.11313 -0.81281 0.6145 -0.44341 -0.19163 -0.089551 -1.5927 0.37405 0.85857 0.54613 -0.31928 0.52598 -1.4802 -0.97931 -0.2939 -0.14724 0.25803 -0.1817 1.0149 0.77649 0.12598 0.54779 -1.0316 0.064599 -0.37523 -0.94475 0.61802 0.39591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading GloVe Embeddings"
      ],
      "metadata": {
        "id": "6Zu3pzUuoFgO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ul9ePwY4EsZ2"
      },
      "outputs": [],
      "source": [
        "def load_glove():\n",
        "\n",
        "    _word2em = {}\n",
        "    file = open(GLOVE_MODEL, mode='r', encoding='utf8')\n",
        "    for line in file:\n",
        "        words = line.strip().split()\n",
        "        word = words[0]\n",
        "        embeds = np.array(words[1:], dtype=np.float32)\n",
        "        _word2em[word] = embeds\n",
        "    file.close()\n",
        "    return _word2em"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3Yd7e37AEsZ3"
      },
      "outputs": [],
      "source": [
        "word2em = load_glove()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contents of the fra.txt"
      ],
      "metadata": {
        "id": "JrnkUSSjnAB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head fra.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaaAr14BI8oj",
        "outputId": "79c9084f-07a5-4e59-a77e-aea5e8a81346"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tVa !\n",
            "Run!\tCours !\n",
            "Run!\tCourez !\n",
            "Wow!\tÇa alors !\n",
            "Fire!\tAu feu !\n",
            "Help!\tÀ l'aide !\n",
            "Jump.\tSaute.\n",
            "Stop!\tÇa suffit !\n",
            "Stop!\tStop !\n",
            "Stop!\tArrête-toi !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0mpplHjJypr",
        "outputId": "3a81a2d7-56e2-4719-d675-8f195c7002b4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Data for Sequence Translation"
      ],
      "metadata": {
        "id": "qZbHHmk3oOAO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "E6BoKdSTEsZ3"
      },
      "outputs": [],
      "source": [
        "lines = open(DATA, 'r', encoding='utf8').read().split('\\n')\n",
        "for line in lines[: len(lines)-1]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    input_words = [w for w in nltk.word_tokenize(input_text.lower())]\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    for char in target_text:\n",
        "        tar_count[char] += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Token Index Dictionaries"
      ],
      "metadata": {
        "id": "T4-mCF_WoWoh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jLboU3vZEsZ4"
      },
      "outputs": [],
      "source": [
        "target_word2idx = dict()\n",
        "\n",
        "for idx, word in enumerate(tar_count.most_common(MAX_VOCAB_SIZE)):\n",
        "    #print(word)\n",
        "    target_word2idx[word[0]] = idx\n",
        "\n",
        "target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
        "\n",
        "num_decoder_tokens = len(target_idx2word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "V1OZHpNwEsZ5"
      },
      "outputs": [],
      "source": [
        "unknown_emb = np.random.randn(EMBEDDING_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5tBaeSK2EsZ5"
      },
      "outputs": [],
      "source": [
        "encoder_max_seq_length = 0\n",
        "decoder_max_seq_length = 0\n",
        "\n",
        "input_texts_word2em = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing Input and Target Texts"
      ],
      "metadata": {
        "id": "pA7JZ7wiodFY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WTq6iSGUEsZ6"
      },
      "outputs": [],
      "source": [
        "lines = open(DATA, 'r', encoding='utf8').read().split('\\n')\n",
        "for line in lines[: min(NUM_SAMPLES, len(lines)-1)]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_words = [w for w in nltk.word_tokenize(input_text.lower())]\n",
        "    encoder_input_wids = []\n",
        "    for w in input_words:\n",
        "        em = unknown_emb\n",
        "        if w in word2em:\n",
        "            em = word2em[w]\n",
        "        encoder_input_wids.append(em)\n",
        "    input_texts_word2em.append(encoder_input_wids)\n",
        "    encoder_max_seq_length = max(len(encoder_input_wids), encoder_max_seq_length)\n",
        "    decoder_max_seq_length = max(len(target_text), decoder_max_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "B8LfFh2oEsZ6"
      },
      "outputs": [],
      "source": [
        "encoder_input_data = pad_sequences(input_texts_word2em, encoder_max_seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## decoder word2index input"
      ],
      "metadata": {
        "id": "px7KQifQm87X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kPT1TtDQEsZ7"
      },
      "outputs": [],
      "source": [
        "decoder_target_data = np.zeros(shape=(NUM_SAMPLES, decoder_max_seq_length, num_decoder_tokens))\n",
        "decoder_input_data = np.zeros(shape=(NUM_SAMPLES, decoder_max_seq_length, num_decoder_tokens))\n",
        "lines = open(DATA, 'rt', encoding='utf8').read().split('\\n')\n",
        "for lineIdx, line in enumerate(lines[: min(NUM_SAMPLES, len(lines)-1)]):\n",
        "    _, target = line.split('\\t')\n",
        "    target = '\\t' + target + '\\n'\n",
        "    for idx, char in enumerate(target):\n",
        "        if char in target_word2idx:\n",
        "            w2idx = target_word2idx[char]\n",
        "            decoder_input_data[lineIdx, idx, w2idx] = 1\n",
        "            if idx > 0:\n",
        "                decoder_target_data[lineIdx, idx-1, w2idx] = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qgMZHQQ4EsZ7"
      },
      "outputs": [],
      "source": [
        "context = dict()\n",
        "context['num_decoder_tokens'] = num_decoder_tokens\n",
        "context['encoder_max_seq_length'] = encoder_max_seq_length\n",
        "context['decoder_max_seq_length'] = decoder_max_seq_length\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## defining Encoder- Decoder Model"
      ],
      "metadata": {
        "id": "H-6nmF0pllyt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KSfKEnlgEsZ7"
      },
      "outputs": [],
      "source": [
        "encoder_inputs = Input(shape=(None, EMBEDDING_SIZE), name='encoder_inputs')\n",
        "encoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, name='encoder_lstm')\n",
        "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_inputs)\n",
        "encoder_states = [encoder_state_h, encoder_state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='decoder_inputs')\n",
        "decoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, name='decoder_lstm')\n",
        "decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs,\n",
        "                                                                 initial_state=encoder_states)\n",
        "decoder_dense = Dense(units=num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X8Uq34nEsZ8",
        "outputId": "811a46f2-b27b-4c33-e498-8687a7f87c55"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "110/110 [==============================] - 68s 525ms/step - loss: 0.9106 - accuracy: 0.1097 - val_loss: 1.0727 - val_accuracy: 0.0844\n",
            "Epoch 2/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "110/110 [==============================] - 62s 561ms/step - loss: 0.8693 - accuracy: 0.0902 - val_loss: 0.9941 - val_accuracy: 0.1076\n",
            "Epoch 3/100\n",
            "110/110 [==============================] - 59s 535ms/step - loss: 0.8225 - accuracy: 0.1029 - val_loss: 0.9636 - val_accuracy: 0.1094\n",
            "Epoch 4/100\n",
            "110/110 [==============================] - 60s 546ms/step - loss: 0.7788 - accuracy: 0.1136 - val_loss: 0.9226 - val_accuracy: 0.1248\n",
            "Epoch 5/100\n",
            "110/110 [==============================] - 73s 665ms/step - loss: 0.7441 - accuracy: 0.1214 - val_loss: 0.8781 - val_accuracy: 0.1338\n",
            "Epoch 6/100\n",
            "110/110 [==============================] - 60s 550ms/step - loss: 0.7099 - accuracy: 0.1302 - val_loss: 0.8874 - val_accuracy: 0.1369\n",
            "Epoch 7/100\n",
            "110/110 [==============================] - 63s 575ms/step - loss: 0.6827 - accuracy: 0.1361 - val_loss: 0.8201 - val_accuracy: 0.1490\n",
            "Epoch 8/100\n",
            "110/110 [==============================] - 57s 515ms/step - loss: 0.6556 - accuracy: 0.1446 - val_loss: 0.7686 - val_accuracy: 0.1639\n",
            "Epoch 9/100\n",
            "110/110 [==============================] - 64s 579ms/step - loss: 0.6303 - accuracy: 0.1559 - val_loss: 0.7616 - val_accuracy: 0.1736\n",
            "Epoch 10/100\n",
            "110/110 [==============================] - 54s 497ms/step - loss: 0.6099 - accuracy: 0.1634 - val_loss: 0.7329 - val_accuracy: 0.1779\n",
            "Epoch 11/100\n",
            "110/110 [==============================] - 55s 501ms/step - loss: 0.5929 - accuracy: 0.1682 - val_loss: 0.7236 - val_accuracy: 0.1809\n",
            "Epoch 12/100\n",
            "110/110 [==============================] - 66s 597ms/step - loss: 0.5754 - accuracy: 0.1722 - val_loss: 0.7176 - val_accuracy: 0.1826\n",
            "Epoch 13/100\n",
            "110/110 [==============================] - 57s 522ms/step - loss: 0.5607 - accuracy: 0.1747 - val_loss: 0.7111 - val_accuracy: 0.1832\n",
            "Epoch 14/100\n",
            "110/110 [==============================] - 58s 524ms/step - loss: 0.5466 - accuracy: 0.1772 - val_loss: 0.6824 - val_accuracy: 0.1881\n",
            "Epoch 15/100\n",
            "110/110 [==============================] - 59s 538ms/step - loss: 0.5296 - accuracy: 0.1805 - val_loss: 0.6782 - val_accuracy: 0.1898\n",
            "Epoch 16/100\n",
            "110/110 [==============================] - 56s 513ms/step - loss: 0.5199 - accuracy: 0.1821 - val_loss: 0.6655 - val_accuracy: 0.1915\n",
            "Epoch 17/100\n",
            "110/110 [==============================] - 58s 523ms/step - loss: 0.5082 - accuracy: 0.1847 - val_loss: 0.6698 - val_accuracy: 0.1900\n",
            "Epoch 18/100\n",
            "110/110 [==============================] - 58s 527ms/step - loss: 0.4986 - accuracy: 0.1863 - val_loss: 0.6549 - val_accuracy: 0.1911\n",
            "Epoch 19/100\n",
            "110/110 [==============================] - 57s 524ms/step - loss: 0.4897 - accuracy: 0.1883 - val_loss: 0.6472 - val_accuracy: 0.1937\n",
            "Epoch 20/100\n",
            "110/110 [==============================] - 61s 555ms/step - loss: 0.4813 - accuracy: 0.1904 - val_loss: 0.6321 - val_accuracy: 0.1969\n",
            "Epoch 21/100\n",
            "110/110 [==============================] - 59s 542ms/step - loss: 0.4734 - accuracy: 0.1921 - val_loss: 0.6336 - val_accuracy: 0.1963\n",
            "Epoch 22/100\n",
            "110/110 [==============================] - 60s 544ms/step - loss: 0.4675 - accuracy: 0.1936 - val_loss: 0.6215 - val_accuracy: 0.1991\n",
            "Epoch 23/100\n",
            "110/110 [==============================] - 58s 527ms/step - loss: 0.4612 - accuracy: 0.1949 - val_loss: 0.6266 - val_accuracy: 0.1970\n",
            "Epoch 24/100\n",
            "110/110 [==============================] - 64s 582ms/step - loss: 0.4565 - accuracy: 0.1964 - val_loss: 0.6208 - val_accuracy: 0.1993\n",
            "Epoch 25/100\n",
            "110/110 [==============================] - 57s 519ms/step - loss: 0.4493 - accuracy: 0.1987 - val_loss: 0.6192 - val_accuracy: 0.1998\n",
            "Epoch 26/100\n",
            "110/110 [==============================] - 70s 636ms/step - loss: 0.4444 - accuracy: 0.2000 - val_loss: 0.6117 - val_accuracy: 0.2011\n",
            "Epoch 27/100\n",
            "110/110 [==============================] - 55s 499ms/step - loss: 0.4394 - accuracy: 0.2016 - val_loss: 0.6137 - val_accuracy: 0.2007\n",
            "Epoch 28/100\n",
            "110/110 [==============================] - 55s 501ms/step - loss: 0.4349 - accuracy: 0.2027 - val_loss: 0.6071 - val_accuracy: 0.2030\n",
            "Epoch 29/100\n",
            "110/110 [==============================] - 68s 619ms/step - loss: 0.4309 - accuracy: 0.2037 - val_loss: 0.6127 - val_accuracy: 0.2010\n",
            "Epoch 30/100\n",
            "110/110 [==============================] - 54s 490ms/step - loss: 0.4258 - accuracy: 0.2052 - val_loss: 0.6147 - val_accuracy: 0.2011\n",
            "Epoch 31/100\n",
            "110/110 [==============================] - 73s 664ms/step - loss: 0.4259 - accuracy: 0.2053 - val_loss: 0.6017 - val_accuracy: 0.2047\n",
            "Epoch 32/100\n",
            "110/110 [==============================] - 57s 523ms/step - loss: 0.4207 - accuracy: 0.2069 - val_loss: 0.6033 - val_accuracy: 0.2041\n",
            "Epoch 33/100\n",
            "110/110 [==============================] - 53s 488ms/step - loss: 0.4142 - accuracy: 0.2089 - val_loss: 0.6043 - val_accuracy: 0.2052\n",
            "Epoch 34/100\n",
            "110/110 [==============================] - 66s 597ms/step - loss: 0.4098 - accuracy: 0.2103 - val_loss: 0.5989 - val_accuracy: 0.2058\n",
            "Epoch 35/100\n",
            "110/110 [==============================] - 53s 481ms/step - loss: 0.4061 - accuracy: 0.2112 - val_loss: 0.5989 - val_accuracy: 0.2059\n",
            "Epoch 36/100\n",
            "110/110 [==============================] - 75s 683ms/step - loss: 0.4042 - accuracy: 0.2117 - val_loss: 0.6104 - val_accuracy: 0.2013\n",
            "Epoch 37/100\n",
            "110/110 [==============================] - 57s 520ms/step - loss: 0.4019 - accuracy: 0.2123 - val_loss: 0.5981 - val_accuracy: 0.2062\n",
            "Epoch 38/100\n",
            "110/110 [==============================] - 54s 497ms/step - loss: 0.3959 - accuracy: 0.2145 - val_loss: 0.5954 - val_accuracy: 0.2062\n",
            "Epoch 39/100\n",
            "110/110 [==============================] - 65s 592ms/step - loss: 0.3941 - accuracy: 0.2148 - val_loss: 0.6016 - val_accuracy: 0.2043\n",
            "Epoch 40/100\n",
            "110/110 [==============================] - 59s 534ms/step - loss: 0.3916 - accuracy: 0.2152 - val_loss: 0.5988 - val_accuracy: 0.2060\n",
            "Epoch 41/100\n",
            "110/110 [==============================] - 63s 577ms/step - loss: 0.3893 - accuracy: 0.2156 - val_loss: 0.6076 - val_accuracy: 0.2028\n",
            "Epoch 42/100\n",
            "110/110 [==============================] - 54s 490ms/step - loss: 0.3860 - accuracy: 0.2169 - val_loss: 0.5985 - val_accuracy: 0.2056\n",
            "Epoch 43/100\n",
            "110/110 [==============================] - 56s 512ms/step - loss: 0.3823 - accuracy: 0.2182 - val_loss: 0.5947 - val_accuracy: 0.2066\n",
            "Epoch 44/100\n",
            "110/110 [==============================] - 63s 577ms/step - loss: 0.3801 - accuracy: 0.2190 - val_loss: 0.6056 - val_accuracy: 0.2032\n",
            "Epoch 45/100\n",
            "110/110 [==============================] - 57s 524ms/step - loss: 0.3794 - accuracy: 0.2191 - val_loss: 0.5920 - val_accuracy: 0.2074\n",
            "Epoch 46/100\n",
            "110/110 [==============================] - 68s 617ms/step - loss: 0.3736 - accuracy: 0.2207 - val_loss: 0.5969 - val_accuracy: 0.2060\n",
            "Epoch 47/100\n",
            "110/110 [==============================] - 57s 520ms/step - loss: 0.3708 - accuracy: 0.2216 - val_loss: 0.5987 - val_accuracy: 0.2043\n",
            "Epoch 48/100\n",
            "110/110 [==============================] - 54s 486ms/step - loss: 0.3666 - accuracy: 0.2229 - val_loss: 0.5965 - val_accuracy: 0.2054\n",
            "Epoch 49/100\n",
            "110/110 [==============================] - 67s 615ms/step - loss: 0.3640 - accuracy: 0.2235 - val_loss: 0.6017 - val_accuracy: 0.2036\n",
            "Epoch 50/100\n",
            "110/110 [==============================] - 55s 503ms/step - loss: 0.3683 - accuracy: 0.2220 - val_loss: 0.5936 - val_accuracy: 0.2061\n",
            "Epoch 51/100\n",
            "110/110 [==============================] - 55s 496ms/step - loss: 0.3592 - accuracy: 0.2249 - val_loss: 0.5917 - val_accuracy: 0.2067\n",
            "Epoch 52/100\n",
            "110/110 [==============================] - 61s 553ms/step - loss: 0.3564 - accuracy: 0.2258 - val_loss: 0.5974 - val_accuracy: 0.2047\n",
            "Epoch 53/100\n",
            "110/110 [==============================] - 53s 488ms/step - loss: 0.3543 - accuracy: 0.2263 - val_loss: 0.5948 - val_accuracy: 0.2063\n",
            "Epoch 54/100\n",
            "110/110 [==============================] - 63s 569ms/step - loss: 0.3563 - accuracy: 0.2257 - val_loss: 0.5936 - val_accuracy: 0.2060\n",
            "Epoch 55/100\n",
            "110/110 [==============================] - 55s 500ms/step - loss: 0.3579 - accuracy: 0.2247 - val_loss: 0.5904 - val_accuracy: 0.2058\n",
            "Epoch 56/100\n",
            "110/110 [==============================] - 59s 535ms/step - loss: 0.3504 - accuracy: 0.2274 - val_loss: 0.5939 - val_accuracy: 0.2068\n",
            "Epoch 57/100\n",
            "110/110 [==============================] - 65s 589ms/step - loss: 0.3464 - accuracy: 0.2286 - val_loss: 0.5904 - val_accuracy: 0.2074\n",
            "Epoch 58/100\n",
            "110/110 [==============================] - 55s 501ms/step - loss: 0.3480 - accuracy: 0.2284 - val_loss: 0.6031 - val_accuracy: 0.2048\n",
            "Epoch 59/100\n",
            "110/110 [==============================] - 58s 528ms/step - loss: 0.3476 - accuracy: 0.2281 - val_loss: 0.5936 - val_accuracy: 0.2066\n",
            "Epoch 60/100\n",
            "110/110 [==============================] - 64s 578ms/step - loss: 0.3397 - accuracy: 0.2309 - val_loss: 0.5993 - val_accuracy: 0.2047\n",
            "Epoch 61/100\n",
            "110/110 [==============================] - 55s 502ms/step - loss: 0.3412 - accuracy: 0.2301 - val_loss: 0.5921 - val_accuracy: 0.2073\n",
            "Epoch 62/100\n",
            "110/110 [==============================] - 72s 656ms/step - loss: 0.3380 - accuracy: 0.2312 - val_loss: 0.5866 - val_accuracy: 0.2091\n",
            "Epoch 63/100\n",
            "110/110 [==============================] - 54s 492ms/step - loss: 0.3444 - accuracy: 0.2290 - val_loss: 0.5874 - val_accuracy: 0.2083\n",
            "Epoch 64/100\n",
            "110/110 [==============================] - 57s 514ms/step - loss: 0.3324 - accuracy: 0.2329 - val_loss: 0.5919 - val_accuracy: 0.2072\n",
            "Epoch 65/100\n",
            "110/110 [==============================] - 68s 614ms/step - loss: 0.3302 - accuracy: 0.2336 - val_loss: 0.5893 - val_accuracy: 0.2079\n",
            "Epoch 66/100\n",
            "110/110 [==============================] - 57s 520ms/step - loss: 0.3277 - accuracy: 0.2344 - val_loss: 0.5917 - val_accuracy: 0.2074\n",
            "Epoch 67/100\n",
            "110/110 [==============================] - 54s 495ms/step - loss: 0.3311 - accuracy: 0.2333 - val_loss: 0.5930 - val_accuracy: 0.2068\n",
            "Epoch 68/100\n",
            "110/110 [==============================] - 65s 586ms/step - loss: 0.3350 - accuracy: 0.2317 - val_loss: 0.6173 - val_accuracy: 0.1986\n",
            "Epoch 69/100\n",
            "110/110 [==============================] - 53s 485ms/step - loss: 0.3525 - accuracy: 0.2261 - val_loss: 0.5936 - val_accuracy: 0.2063\n",
            "Epoch 70/100\n",
            "110/110 [==============================] - 61s 555ms/step - loss: 0.3284 - accuracy: 0.2341 - val_loss: 0.5867 - val_accuracy: 0.2086\n",
            "Epoch 71/100\n",
            "110/110 [==============================] - 53s 483ms/step - loss: 0.3244 - accuracy: 0.2355 - val_loss: 0.5879 - val_accuracy: 0.2081\n",
            "Epoch 72/100\n",
            "110/110 [==============================] - 57s 519ms/step - loss: 0.3229 - accuracy: 0.2360 - val_loss: 0.5919 - val_accuracy: 0.2073\n",
            "Epoch 73/100\n",
            "110/110 [==============================] - 64s 589ms/step - loss: 0.3177 - accuracy: 0.2371 - val_loss: 0.5895 - val_accuracy: 0.2078\n",
            "Epoch 74/100\n",
            "110/110 [==============================] - 52s 474ms/step - loss: 0.3200 - accuracy: 0.2366 - val_loss: 0.5979 - val_accuracy: 0.2049\n",
            "Epoch 75/100\n",
            "110/110 [==============================] - 55s 500ms/step - loss: 0.3132 - accuracy: 0.2387 - val_loss: 0.5889 - val_accuracy: 0.2078\n",
            "Epoch 76/100\n",
            "110/110 [==============================] - 67s 611ms/step - loss: 0.3139 - accuracy: 0.2387 - val_loss: 0.5953 - val_accuracy: 0.2059\n",
            "Epoch 77/100\n",
            "110/110 [==============================] - 52s 477ms/step - loss: 0.3188 - accuracy: 0.2365 - val_loss: 0.5963 - val_accuracy: 0.2060\n",
            "Epoch 78/100\n",
            "110/110 [==============================] - 57s 519ms/step - loss: 0.3137 - accuracy: 0.2382 - val_loss: 0.6001 - val_accuracy: 0.2047\n",
            "Epoch 79/100\n",
            "110/110 [==============================] - 63s 574ms/step - loss: 0.3120 - accuracy: 0.2389 - val_loss: 0.5920 - val_accuracy: 0.2072\n",
            "Epoch 80/100\n",
            "110/110 [==============================] - 55s 504ms/step - loss: 0.3073 - accuracy: 0.2402 - val_loss: 0.5920 - val_accuracy: 0.2072\n",
            "Epoch 81/100\n",
            "110/110 [==============================] - 61s 553ms/step - loss: 0.3047 - accuracy: 0.2414 - val_loss: 0.5976 - val_accuracy: 0.2055\n",
            "Epoch 82/100\n",
            "110/110 [==============================] - 55s 499ms/step - loss: 0.3063 - accuracy: 0.2407 - val_loss: 0.5894 - val_accuracy: 0.2084\n",
            "Epoch 83/100\n",
            "110/110 [==============================] - 58s 532ms/step - loss: 0.3039 - accuracy: 0.2415 - val_loss: 0.5930 - val_accuracy: 0.2076\n",
            "Epoch 84/100\n",
            "110/110 [==============================] - 64s 581ms/step - loss: 0.3054 - accuracy: 0.2407 - val_loss: 0.5905 - val_accuracy: 0.2086\n",
            "Epoch 85/100\n",
            "110/110 [==============================] - 57s 517ms/step - loss: 0.2993 - accuracy: 0.2428 - val_loss: 0.5966 - val_accuracy: 0.2064\n",
            "Epoch 86/100\n",
            "110/110 [==============================] - 53s 479ms/step - loss: 0.3044 - accuracy: 0.2410 - val_loss: 0.5981 - val_accuracy: 0.2055\n",
            "Epoch 87/100\n",
            "110/110 [==============================] - 61s 560ms/step - loss: 0.3052 - accuracy: 0.2407 - val_loss: 0.5955 - val_accuracy: 0.2060\n",
            "Epoch 88/100\n",
            "110/110 [==============================] - 53s 479ms/step - loss: 0.3333 - accuracy: 0.2315 - val_loss: 0.5957 - val_accuracy: 0.2062\n",
            "Epoch 89/100\n",
            "110/110 [==============================] - 57s 520ms/step - loss: 0.3057 - accuracy: 0.2407 - val_loss: 0.5953 - val_accuracy: 0.2053\n",
            "Epoch 90/100\n",
            "110/110 [==============================] - 63s 572ms/step - loss: 0.2967 - accuracy: 0.2436 - val_loss: 0.5921 - val_accuracy: 0.2074\n",
            "Epoch 91/100\n",
            "110/110 [==============================] - 58s 531ms/step - loss: 0.2908 - accuracy: 0.2457 - val_loss: 0.5911 - val_accuracy: 0.2077\n",
            "Epoch 92/100\n",
            "110/110 [==============================] - 60s 549ms/step - loss: 0.2962 - accuracy: 0.2434 - val_loss: 0.5978 - val_accuracy: 0.2059\n",
            "Epoch 93/100\n",
            "110/110 [==============================] - 59s 532ms/step - loss: 0.2975 - accuracy: 0.2429 - val_loss: 0.5970 - val_accuracy: 0.2057\n",
            "Epoch 94/100\n",
            "110/110 [==============================] - 55s 499ms/step - loss: 0.2891 - accuracy: 0.2458 - val_loss: 0.5952 - val_accuracy: 0.2064\n",
            "Epoch 95/100\n",
            "110/110 [==============================] - 62s 565ms/step - loss: 0.2882 - accuracy: 0.2462 - val_loss: 0.5966 - val_accuracy: 0.2061\n",
            "Epoch 96/100\n",
            "110/110 [==============================] - 54s 494ms/step - loss: 0.3020 - accuracy: 0.2415 - val_loss: 0.6077 - val_accuracy: 0.2022\n",
            "Epoch 97/100\n",
            "110/110 [==============================] - 58s 533ms/step - loss: 0.3071 - accuracy: 0.2394 - val_loss: 0.5997 - val_accuracy: 0.2061\n",
            "Epoch 98/100\n",
            "110/110 [==============================] - 62s 570ms/step - loss: 0.2934 - accuracy: 0.2442 - val_loss: 0.5961 - val_accuracy: 0.2063\n",
            "Epoch 99/100\n",
            "110/110 [==============================] - 53s 478ms/step - loss: 0.2865 - accuracy: 0.2470 - val_loss: 0.5996 - val_accuracy: 0.2050\n",
            "Epoch 100/100\n",
            "110/110 [==============================] - 57s 518ms/step - loss: 0.2841 - accuracy: 0.2476 - val_loss: 0.5989 - val_accuracy: 0.2055\n"
          ]
        }
      ],
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=WEIGHT_FILE_PATH, save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
        "          verbose=1, validation_split=0.3, callbacks=[checkpoint])\n",
        "\n",
        "model.save_weights(WEIGHT_FILE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vvi-dIXAEsZ8",
        "outputId": "02ea82b1-2f8b-4f64-b718-7c9209cda675"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer  [(None, None, 100)]          0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer  [(None, None, 110)]          0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " encoder_lstm (LSTM)         [(None, 256),                365568    ['encoder_inputs[0][0]']      \n",
            "                              (None, 256),                                                        \n",
            "                              (None, 256)]                                                        \n",
            "                                                                                                  \n",
            " decoder_lstm (LSTM)         [(None, None, 256),          375808    ['decoder_inputs[0][0]',      \n",
            "                              (None, 256),                           'encoder_lstm[0][1]',        \n",
            "                              (None, 256)]                           'encoder_lstm[0][2]']        \n",
            "                                                                                                  \n",
            " decoder_dense (Dense)       (None, None, 110)            28270     ['decoder_lstm[0][0]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 769646 (2.94 MB)\n",
            "Trainable params: 769646 (2.94 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "KB0DUwZXEsZ9"
      },
      "outputs": [],
      "source": [
        "encoder_model_inf = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "\n",
        "decoder_state_input_h = Input(shape=(HIDDEN_UNITS,))\n",
        "decoder_state_input_c = Input(shape=(HIDDEN_UNITS,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_lstm(decoder_inputs,\n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_inputs] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ybBqcoo4EsZ9"
      },
      "outputs": [],
      "source": [
        "max_encoder_seq_length = context['encoder_max_seq_length']\n",
        "max_decoder_seq_length = context['decoder_max_seq_length']\n",
        "num_decoder_tokens = context['num_decoder_tokens']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "R_44M9cfEsZ-"
      },
      "outputs": [],
      "source": [
        "def predict_sent(input_text):\n",
        "        input_seq = []\n",
        "        input_wids = []\n",
        "        for word in nltk.word_tokenize(input_text.lower()):\n",
        "            emb = unknown_emb\n",
        "            if word in word2em:\n",
        "                emb = word2em[word]\n",
        "            input_wids.append(emb)\n",
        "        input_seq.append(input_wids)\n",
        "        input_seq = pad_sequences(input_seq, max_encoder_seq_length)\n",
        "        states_value = encoder_model_inf.predict(input_seq)\n",
        "        target_seq = np.zeros((1, 1,num_decoder_tokens))\n",
        "        target_seq[0, 0, target_word2idx['\\t']] = 1\n",
        "        target_text = ''\n",
        "        terminated = False\n",
        "        while not terminated:\n",
        "            output_tokens, h, c = decoder_model_inf.predict([target_seq] + states_value)\n",
        "\n",
        "            sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
        "            sample_word = target_idx2word[sample_token_idx]\n",
        "            target_text += sample_word\n",
        "\n",
        "            if sample_word == '\\n' or len(target_text) >= max_decoder_seq_length:\n",
        "                terminated = True\n",
        "\n",
        "            target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "            target_seq[0, 0, sample_token_idx] = 1\n",
        "\n",
        "            states_value = [h, c]\n",
        "        return target_text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAGMm3ytEsZ-",
        "outputId": "b88585b8-5fec-444c-fcdf-437671e56473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 161ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "J'aime l'inte.\n"
          ]
        }
      ],
      "source": [
        "print(predict_sent('I like music'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "_-IBxVW4EsZ-"
      },
      "outputs": [],
      "source": [
        "def calculate_bleu_score(reference, candidate):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    return sentence_bleu([reference], candidate, smoothing_function=smoothie)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = 'I like music.'\n",
        "predicted_sentence = predict_sent(input_sentence)\n",
        "print('Predicted:', predicted_sentence)\n",
        "\n",
        "reference_sentence = 'J\\'aime la musique.'  # Reference translation\n",
        "bleu_score = calculate_bleu_score(reference_sentence.split(), predicted_sentence.split())\n",
        "print('BLEU score:', bleu_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsbPKlftmNp4",
        "outputId": "8c632575-e3ab-4757-aec1-97d1202570b7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 175ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Predicted: J'aime le chante.\n",
            "BLEU score: 0.07249749990681824\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:tfp3.6]",
      "language": "python",
      "name": "conda-env-tfp3.6-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}