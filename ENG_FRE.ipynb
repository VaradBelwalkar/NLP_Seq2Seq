{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VaradBelwalkar/NLP_Seq2Seq/blob/main/ENG_FRE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## English to French Seq2Seq model"
      ],
      "metadata": {
        "id": "a6YDaLolkco4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "FQCmps7kEsZt"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense, Input, Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from collections import Counter\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile\n",
        "import sys\n",
        "import urllib.request\n",
        "from gensim.models import KeyedVectors\n",
        "from keras.utils import plot_model\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "evak6GnuEsZw"
      },
      "outputs": [],
      "source": [
        "from keras.activations import softmax\n",
        "from keras.layers import Dense, Activation, RepeatVector, Permute\n",
        "from keras.layers import Input, Embedding, Multiply, Concatenate, Lambda\n",
        "from keras.layers import TimeDistributed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JYHPE4NEsZx"
      },
      "source": [
        "#hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "pD7U6tK4EsZz"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 100\n",
        "HIDDEN_UNITS = 256\n",
        "NUM_SAMPLES = 10000\n",
        "MAX_VOCAB_SIZE = 10000\n",
        "EMBEDDING_SIZE = 100\n",
        "DATA = 'fra.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "P1Jj6A5PEsZ1"
      },
      "outputs": [],
      "source": [
        "tar_count = Counter()\n",
        "\n",
        "GLOVE_MODEL = \"glove.6B.100d.txt\"\n",
        "WEIGHT_FILE_PATH = 'eng-to-fr-glove-weights.h5'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the GloVe embeddings zip"
      ],
      "metadata": {
        "id": "RCjsSYtwnXob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VXLT49jF91Y",
        "outputId": "5d950f9c-5f50-4d08-9db7-c57c1a8a661f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-08 09:16:21--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-06-08 09:16:21--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-06-08 09:16:21--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2024-06-08 09:19:01 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting the zip to get required embeddings"
      ],
      "metadata": {
        "id": "15eB_nKbneyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWV_9c86HyhT",
        "outputId": "f2ce7c9e-3de3-4e81-ed80-37c9cc5333ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample word embeddings"
      ],
      "metadata": {
        "id": "LcQb2_TwnjjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head glove.6B.100d.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWvHayPMISLU",
        "outputId": "ab9aa0dd-03e8-4cdb-cc7d-25017a4110f2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n",
            ", -0.10767 0.11053 0.59812 -0.54361 0.67396 0.10663 0.038867 0.35481 0.06351 -0.094189 0.15786 -0.81665 0.14172 0.21939 0.58505 -0.52158 0.22783 -0.16642 -0.68228 0.3587 0.42568 0.19021 0.91963 0.57555 0.46185 0.42363 -0.095399 -0.42749 -0.16567 -0.056842 -0.29595 0.26037 -0.26606 -0.070404 -0.27662 0.15821 0.69825 0.43081 0.27952 -0.45437 -0.33801 -0.58184 0.22364 -0.5778 -0.26862 -0.20425 0.56394 -0.58524 -0.14365 -0.64218 0.0054697 -0.35248 0.16162 1.1796 -0.47674 -2.7553 -0.1321 -0.047729 1.0655 1.1034 -0.2208 0.18669 0.13177 0.15117 0.7131 -0.35215 0.91348 0.61783 0.70992 0.23955 -0.14571 -0.37859 -0.045959 -0.47368 0.2385 0.20536 -0.18996 0.32507 -1.1112 -0.36341 0.98679 -0.084776 -0.54008 0.11726 -1.0194 -0.24424 0.12771 0.013884 0.080374 -0.35414 0.34951 -0.7226 0.37549 0.4441 -0.99059 0.61214 -0.35111 -0.83155 0.45293 0.082577\n",
            ". -0.33979 0.20941 0.46348 -0.64792 -0.38377 0.038034 0.17127 0.15978 0.46619 -0.019169 0.41479 -0.34349 0.26872 0.04464 0.42131 -0.41032 0.15459 0.022239 -0.64653 0.25256 0.043136 -0.19445 0.46516 0.45651 0.68588 0.091295 0.21875 -0.70351 0.16785 -0.35079 -0.12634 0.66384 -0.2582 0.036542 -0.13605 0.40253 0.14289 0.38132 -0.12283 -0.45886 -0.25282 -0.30432 -0.11215 -0.26182 -0.22482 -0.44554 0.2991 -0.85612 -0.14503 -0.49086 0.0082973 -0.17491 0.27524 1.4401 -0.21239 -2.8435 -0.27958 -0.45722 1.6386 0.78808 -0.55262 0.65 0.086426 0.39012 1.0632 -0.35379 0.48328 0.346 0.84174 0.098707 -0.24213 -0.27053 0.045287 -0.40147 0.11395 0.0062226 0.036673 0.018518 -1.0213 -0.20806 0.64072 -0.068763 -0.58635 0.33476 -1.1432 -0.1148 -0.25091 -0.45907 -0.096819 -0.17946 -0.063351 -0.67412 -0.068895 0.53604 -0.87773 0.31802 -0.39242 -0.23394 0.47298 -0.028803\n",
            "of -0.1529 -0.24279 0.89837 0.16996 0.53516 0.48784 -0.58826 -0.17982 -1.3581 0.42541 0.15377 0.24215 0.13474 0.41193 0.67043 -0.56418 0.42985 -0.012183 -0.11677 0.31781 0.054177 -0.054273 0.35516 -0.30241 0.31434 -0.33846 0.71715 -0.26855 -0.15837 -0.47467 0.051581 -0.33252 0.15003 -0.1299 -0.54617 -0.37843 0.64261 0.82187 -0.080006 0.078479 -0.96976 -0.57741 0.56491 -0.39873 -0.057099 0.19743 0.065706 -0.48092 -0.20125 -0.40834 0.39456 -0.02642 -0.11838 1.012 -0.53171 -2.7474 -0.042981 -0.74849 1.7574 0.59085 0.04885 0.78267 0.38497 0.42097 0.67882 0.10337 0.6328 -0.026595 0.58647 -0.44332 0.33057 -0.12022 -0.55645 0.073611 0.20915 0.43395 -0.012761 0.089874 -1.7991 0.084808 0.77112 0.63105 -0.90685 0.60326 -1.7515 0.18596 -0.50687 -0.70203 0.66578 -0.81304 0.18712 -0.018488 -0.26757 0.727 -0.59363 -0.34839 -0.56094 -0.591 1.0039 0.20664\n",
            "to -0.1897 0.050024 0.19084 -0.049184 -0.089737 0.21006 -0.54952 0.098377 -0.20135 0.34241 -0.092677 0.161 -0.13268 -0.2816 0.18737 -0.42959 0.96039 0.13972 -1.0781 0.40518 0.50539 -0.55064 0.4844 0.38044 -0.0029055 -0.34942 -0.099696 -0.78368 1.0363 -0.2314 -0.47121 0.57126 -0.21454 0.35958 -0.48319 1.0875 0.28524 0.12447 -0.039248 -0.076732 -0.76343 -0.32409 -0.5749 -1.0893 -0.41811 0.4512 0.12112 -0.51367 -0.13349 -1.1378 -0.28768 0.16774 0.55804 1.5387 0.018859 -2.9721 -0.24216 -0.92495 2.1992 0.28234 -0.3478 0.51621 -0.43387 0.36852 0.74573 0.072102 0.27931 0.92569 -0.050336 -0.85856 -0.1358 -0.92551 -0.33991 -1.0394 -0.067203 -0.21379 -0.4769 0.21377 -0.84008 0.052536 0.59298 0.29604 -0.67644 0.13916 -1.5504 -0.20765 0.7222 0.52056 -0.076221 -0.15194 -0.13134 0.058617 -0.31869 -0.61419 -0.62393 -0.41548 -0.038175 -0.39804 0.47647 -0.15983\n",
            "and -0.071953 0.23127 0.023731 -0.50638 0.33923 0.1959 -0.32943 0.18364 -0.18057 0.28963 0.20448 -0.5496 0.27399 0.58327 0.20468 -0.49228 0.19974 -0.070237 -0.88049 0.29485 0.14071 -0.1009 0.99449 0.36973 0.44554 0.28998 -0.1376 -0.56365 -0.029365 -0.4122 -0.25269 0.63181 -0.44767 0.24363 -0.10813 0.25164 0.46967 0.3755 -0.23613 -0.14129 -0.44537 -0.65737 -0.042421 -0.28636 -0.28811 0.063766 0.20281 -0.53542 0.41307 -0.59722 -0.38614 0.19389 -0.17809 1.6618 -0.011819 -2.3737 0.058427 -0.2698 1.2823 0.81925 -0.22322 0.72932 -0.053211 0.43507 0.85011 -0.42935 0.92664 0.39051 1.0585 -0.24561 -0.18265 -0.5328 0.059518 -0.66019 0.18991 0.28836 -0.2434 0.52784 -0.65762 -0.14081 1.0491 0.5134 -0.23816 0.69895 -1.4813 -0.2487 -0.17936 -0.059137 -0.08056 -0.48782 0.014487 -0.6259 -0.32367 0.41862 -1.0807 0.46742 -0.49931 -0.71895 0.86894 0.19539\n",
            "in 0.085703 -0.22201 0.16569 0.13373 0.38239 0.35401 0.01287 0.22461 -0.43817 0.50164 -0.35874 -0.34983 0.055156 0.69648 -0.17958 0.067926 0.39101 0.16039 -0.26635 -0.21138 0.53698 0.49379 0.9366 0.66902 0.21793 -0.46642 0.22383 -0.36204 -0.17656 0.1748 -0.20367 0.13931 0.019832 -0.10413 -0.20244 0.55003 -0.1546 0.98655 -0.26863 -0.2909 -0.32866 -0.34188 -0.16943 -0.42001 -0.046727 -0.16327 0.70824 -0.74911 -0.091559 -0.96178 -0.19747 0.10282 0.55221 1.3816 -0.65636 -3.2502 -0.31556 -1.2055 1.7709 0.4026 -0.79827 1.1597 -0.33042 0.31382 0.77386 0.22595 0.52471 -0.034053 0.32048 0.079948 0.17752 -0.49426 -0.70045 -0.44569 0.17244 0.20278 0.023292 -0.20677 -1.0158 0.18325 0.56752 0.31821 -0.65011 0.68277 -0.86585 -0.059392 -0.29264 -0.55668 -0.34705 -0.32895 0.40215 -0.12746 -0.20228 0.87368 -0.545 0.79205 -0.20695 -0.074273 0.75808 -0.34243\n",
            "a -0.27086 0.044006 -0.02026 -0.17395 0.6444 0.71213 0.3551 0.47138 -0.29637 0.54427 -0.72294 -0.0047612 0.040611 0.043236 0.29729 0.10725 0.40156 -0.53662 0.033382 0.067396 0.64556 -0.085523 0.14103 0.094539 0.74947 -0.194 -0.68739 -0.41741 -0.22807 0.12 -0.48999 0.80945 0.045138 -0.11898 0.20161 0.39276 -0.20121 0.31354 0.75304 0.25907 -0.11566 -0.029319 0.93499 -0.36067 0.5242 0.23706 0.52715 0.22869 -0.51958 -0.79349 -0.20368 -0.50187 0.18748 0.94282 -0.44834 -3.6792 0.044183 -0.26751 2.1997 0.241 -0.033425 0.69553 -0.64472 -0.0072277 0.89575 0.20015 0.46493 0.61933 -0.1066 0.08691 -0.4623 0.18262 -0.15849 0.020791 0.19373 0.063426 -0.31673 -0.48177 -1.3848 0.13669 0.96859 0.049965 -0.2738 -0.035686 -1.0577 -0.24467 0.90366 -0.12442 0.080776 -0.83401 0.57201 0.088945 -0.42532 -0.018253 -0.079995 -0.28581 -0.01089 -0.4923 0.63687 0.23642\n",
            "\" -0.30457 -0.23645 0.17576 -0.72854 -0.28343 -0.2564 0.26587 0.025309 -0.074775 -0.3766 -0.057774 0.12159 0.34384 0.41928 -0.23236 -0.31547 0.60939 0.25117 -0.68667 0.70873 1.2162 -0.1824 -0.48442 -0.33445 0.30343 1.086 0.49992 -0.20198 0.27959 0.68352 -0.33566 -0.12405 0.059656 0.33617 0.37501 0.56552 0.44867 0.11284 -0.16196 -0.94346 -0.67961 0.18581 0.060653 0.43776 0.13834 -0.48207 -0.56141 -0.25422 -0.52445 0.097003 -0.48925 0.19077 0.21481 1.4969 -0.86665 -3.2846 0.56854 0.41971 1.2294 0.78522 -0.29369 0.63803 -1.5926 -0.20437 1.5306 0.13548 0.50722 0.18742 0.48552 -0.28995 0.19573 0.0046515 0.092879 -0.42444 0.64987 0.52839 0.077908 0.8263 -1.2208 -0.34955 0.49855 -0.64155 -0.72308 0.26566 -1.3643 -0.46364 -0.52048 -1.0525 0.22895 -0.3456 -0.658 -0.16735 0.35158 0.74337 0.26074 0.061104 -0.39079 -0.84557 -0.035432 0.17036\n",
            "'s 0.58854 -0.2025 0.73479 -0.68338 -0.19675 -0.1802 -0.39177 0.34172 -0.60561 0.63816 -0.26695 0.36486 -0.40379 -0.1134 -0.58718 0.2838 0.8025 -0.35303 0.30083 0.078935 0.44416 -0.45906 0.79294 0.50365 0.32805 0.28027 -0.4933 -0.38482 -0.039284 -0.2483 -0.1988 1.1469 0.13228 0.91691 -0.36739 0.89425 0.5426 0.61738 -0.62205 -0.31132 -0.50933 0.23335 1.0826 -0.044637 -0.12767 0.27628 -0.032617 -0.27397 0.77764 -0.50861 0.038307 -0.33679 0.42344 1.2271 -0.53826 -3.2411 0.42626 0.025189 1.3948 0.65085 0.03325 0.37141 0.4044 0.35558 0.98265 -0.61724 0.53901 0.76219 0.30689 0.33065 0.30956 -0.15161 -0.11313 -0.81281 0.6145 -0.44341 -0.19163 -0.089551 -1.5927 0.37405 0.85857 0.54613 -0.31928 0.52598 -1.4802 -0.97931 -0.2939 -0.14724 0.25803 -0.1817 1.0149 0.77649 0.12598 0.54779 -1.0316 0.064599 -0.37523 -0.94475 0.61802 0.39591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading GloVe Embeddings"
      ],
      "metadata": {
        "id": "6Zu3pzUuoFgO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "ul9ePwY4EsZ2"
      },
      "outputs": [],
      "source": [
        "def load_glove():\n",
        "\n",
        "    _word2em = {}\n",
        "    file = open(GLOVE_MODEL, mode='r', encoding='utf8')\n",
        "    for line in file:\n",
        "        words = line.strip().split()\n",
        "        word = words[0]\n",
        "        embeds = np.array(words[1:], dtype=np.float32)\n",
        "        _word2em[word] = embeds\n",
        "    file.close()\n",
        "    return _word2em"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "3Yd7e37AEsZ3"
      },
      "outputs": [],
      "source": [
        "word2em = load_glove()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contents of the fra.txt"
      ],
      "metadata": {
        "id": "JrnkUSSjnAB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head fra.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaaAr14BI8oj",
        "outputId": "cec69f55-5f6e-4d52-d201-087327937df8"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tVa !\n",
            "Run!\tCours !\n",
            "Run!\tCourez !\n",
            "Wow!\tÇa alors !\n",
            "Fire!\tAu feu !\n",
            "Help!\tÀ l'aide !\n",
            "Jump.\tSaute.\n",
            "Stop!\tÇa suffit !\n",
            "Stop!\tStop !\n",
            "Stop!\tArrête-toi !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0mpplHjJypr",
        "outputId": "db5b1329-b3a2-4951-b107-feb8a6b92bb3"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Data for Sequence Translation"
      ],
      "metadata": {
        "id": "qZbHHmk3oOAO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "E6BoKdSTEsZ3"
      },
      "outputs": [],
      "source": [
        "lines = open(DATA, 'r', encoding='utf8').read().split('\\n')\n",
        "for line in lines[: len(lines)-1]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    input_words = [w for w in nltk.word_tokenize(input_text.lower())]\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    for char in target_text:\n",
        "        tar_count[char] += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Token Index Dictionaries"
      ],
      "metadata": {
        "id": "T4-mCF_WoWoh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "jLboU3vZEsZ4"
      },
      "outputs": [],
      "source": [
        "target_word2idx = dict()\n",
        "\n",
        "for idx, word in enumerate(tar_count.most_common(MAX_VOCAB_SIZE)):\n",
        "    #print(word)\n",
        "    target_word2idx[word[0]] = idx\n",
        "\n",
        "target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
        "\n",
        "num_decoder_tokens = len(target_idx2word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "V1OZHpNwEsZ5"
      },
      "outputs": [],
      "source": [
        "unknown_emb = np.random.randn(EMBEDDING_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "5tBaeSK2EsZ5"
      },
      "outputs": [],
      "source": [
        "encoder_max_seq_length = 0\n",
        "decoder_max_seq_length = 0\n",
        "\n",
        "input_texts_word2em = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing Input and Target Texts"
      ],
      "metadata": {
        "id": "pA7JZ7wiodFY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "WTq6iSGUEsZ6"
      },
      "outputs": [],
      "source": [
        "lines = open(DATA, 'r', encoding='utf8').read().split('\\n')\n",
        "for line in lines[: min(NUM_SAMPLES, len(lines)-1)]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_words = [w for w in nltk.word_tokenize(input_text.lower())]\n",
        "    encoder_input_wids = []\n",
        "    for w in input_words:\n",
        "        em = unknown_emb\n",
        "        if w in word2em:\n",
        "            em = word2em[w]\n",
        "        encoder_input_wids.append(em)\n",
        "    input_texts_word2em.append(encoder_input_wids)\n",
        "    encoder_max_seq_length = max(len(encoder_input_wids), encoder_max_seq_length)\n",
        "    decoder_max_seq_length = max(len(target_text), decoder_max_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "B8LfFh2oEsZ6"
      },
      "outputs": [],
      "source": [
        "encoder_input_data = pad_sequences(input_texts_word2em, encoder_max_seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## decoder word2index input"
      ],
      "metadata": {
        "id": "px7KQifQm87X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "kPT1TtDQEsZ7"
      },
      "outputs": [],
      "source": [
        "decoder_target_data = np.zeros(shape=(NUM_SAMPLES, decoder_max_seq_length, num_decoder_tokens))\n",
        "decoder_input_data = np.zeros(shape=(NUM_SAMPLES, decoder_max_seq_length, num_decoder_tokens))\n",
        "lines = open(DATA, 'rt', encoding='utf8').read().split('\\n')\n",
        "for lineIdx, line in enumerate(lines[: min(NUM_SAMPLES, len(lines)-1)]):\n",
        "    _, target = line.split('\\t')\n",
        "    target = '\\t' + target + '\\n'\n",
        "    for idx, char in enumerate(target):\n",
        "        if char in target_word2idx:\n",
        "            w2idx = target_word2idx[char]\n",
        "            decoder_input_data[lineIdx, idx, w2idx] = 1\n",
        "            if idx > 0:\n",
        "                decoder_target_data[lineIdx, idx-1, w2idx] = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "qgMZHQQ4EsZ7"
      },
      "outputs": [],
      "source": [
        "context = dict()\n",
        "context['num_decoder_tokens'] = num_decoder_tokens\n",
        "context['encoder_max_seq_length'] = encoder_max_seq_length\n",
        "context['decoder_max_seq_length'] = decoder_max_seq_length\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## defining Encoder- Decoder Model"
      ],
      "metadata": {
        "id": "H-6nmF0pllyt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "KSfKEnlgEsZ7"
      },
      "outputs": [],
      "source": [
        "encoder_inputs = Input(shape=(None, EMBEDDING_SIZE), name='encoder_inputs')\n",
        "encoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, name='encoder_lstm')\n",
        "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_inputs)\n",
        "encoder_states = [encoder_state_h, encoder_state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='decoder_inputs')\n",
        "decoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, name='decoder_lstm')\n",
        "decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs,\n",
        "                                                                 initial_state=encoder_states)\n",
        "decoder_dense = Dense(units=num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X8Uq34nEsZ8",
        "outputId": "acf0da26-b608-4be2-e4ee-288cf0dba2c1"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "110/110 [==============================] - 67s 578ms/step - loss: 1.0569 - val_loss: 1.1666\n",
            "Epoch 2/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "110/110 [==============================] - 67s 612ms/step - loss: 0.9791 - val_loss: 1.1310\n",
            "Epoch 3/100\n",
            "110/110 [==============================] - 55s 503ms/step - loss: 0.9455 - val_loss: 1.0936\n",
            "Epoch 4/100\n",
            "110/110 [==============================] - 62s 566ms/step - loss: 0.9153 - val_loss: 1.0708\n",
            "Epoch 5/100\n",
            "110/110 [==============================] - 58s 533ms/step - loss: 0.8834 - val_loss: 1.0299\n",
            "Epoch 6/100\n",
            "110/110 [==============================] - 68s 618ms/step - loss: 0.8509 - val_loss: 0.9855\n",
            "Epoch 7/100\n",
            "110/110 [==============================] - 60s 543ms/step - loss: 0.8044 - val_loss: 0.9293\n",
            "Epoch 8/100\n",
            "110/110 [==============================] - 54s 497ms/step - loss: 0.7636 - val_loss: 0.8927\n",
            "Epoch 9/100\n",
            "110/110 [==============================] - 66s 602ms/step - loss: 0.7273 - val_loss: 0.8538\n",
            "Epoch 10/100\n",
            "110/110 [==============================] - 63s 571ms/step - loss: 0.6976 - val_loss: 0.8183\n",
            "Epoch 11/100\n",
            "110/110 [==============================] - 54s 497ms/step - loss: 0.6725 - val_loss: 0.8084\n",
            "Epoch 12/100\n",
            "110/110 [==============================] - 53s 477ms/step - loss: 0.6521 - val_loss: 0.7756\n",
            "Epoch 13/100\n",
            "110/110 [==============================] - 73s 666ms/step - loss: 0.6344 - val_loss: 0.7782\n",
            "Epoch 14/100\n",
            "110/110 [==============================] - 54s 494ms/step - loss: 0.6208 - val_loss: 0.7593\n",
            "Epoch 15/100\n",
            "110/110 [==============================] - 53s 480ms/step - loss: 0.6064 - val_loss: 0.7386\n",
            "Epoch 16/100\n",
            "110/110 [==============================] - 59s 541ms/step - loss: 0.5944 - val_loss: 0.7329\n",
            "Epoch 17/100\n",
            "110/110 [==============================] - 65s 596ms/step - loss: 0.5849 - val_loss: 0.7326\n",
            "Epoch 18/100\n",
            "110/110 [==============================] - 55s 497ms/step - loss: 0.5778 - val_loss: 0.7172\n",
            "Epoch 19/100\n",
            "110/110 [==============================] - 57s 524ms/step - loss: 0.5683 - val_loss: 0.7029\n",
            "Epoch 20/100\n",
            "110/110 [==============================] - 57s 520ms/step - loss: 0.5599 - val_loss: 0.6974\n",
            "Epoch 21/100\n",
            "110/110 [==============================] - 65s 589ms/step - loss: 0.5523 - val_loss: 0.7058\n",
            "Epoch 22/100\n",
            "110/110 [==============================] - 54s 489ms/step - loss: 0.5439 - val_loss: 0.6998\n",
            "Epoch 23/100\n",
            "110/110 [==============================] - 55s 497ms/step - loss: 0.5379 - val_loss: 0.6896\n",
            "Epoch 24/100\n",
            "110/110 [==============================] - 65s 597ms/step - loss: 0.5303 - val_loss: 0.6806\n",
            "Epoch 25/100\n",
            "110/110 [==============================] - 59s 535ms/step - loss: 0.5241 - val_loss: 0.6783\n",
            "Epoch 26/100\n",
            "110/110 [==============================] - 57s 516ms/step - loss: 0.5160 - val_loss: 0.6804\n",
            "Epoch 27/100\n",
            "110/110 [==============================] - 57s 519ms/step - loss: 0.5109 - val_loss: 0.6637\n",
            "Epoch 28/100\n",
            "110/110 [==============================] - 66s 603ms/step - loss: 0.5056 - val_loss: 0.6710\n",
            "Epoch 29/100\n",
            "110/110 [==============================] - 59s 539ms/step - loss: 0.5063 - val_loss: 0.6662\n",
            "Epoch 30/100\n",
            "110/110 [==============================] - 54s 491ms/step - loss: 0.4939 - val_loss: 0.6634\n",
            "Epoch 31/100\n",
            "110/110 [==============================] - 59s 538ms/step - loss: 0.4872 - val_loss: 0.6698\n",
            "Epoch 32/100\n",
            "110/110 [==============================] - 65s 588ms/step - loss: 0.4838 - val_loss: 0.6521\n",
            "Epoch 33/100\n",
            "110/110 [==============================] - 55s 499ms/step - loss: 0.4760 - val_loss: 0.6595\n",
            "Epoch 34/100\n",
            "110/110 [==============================] - 56s 511ms/step - loss: 0.4700 - val_loss: 0.6508\n",
            "Epoch 35/100\n",
            "110/110 [==============================] - 59s 543ms/step - loss: 0.4636 - val_loss: 0.6506\n",
            "Epoch 36/100\n",
            "110/110 [==============================] - 58s 526ms/step - loss: 0.4636 - val_loss: 0.6437\n",
            "Epoch 37/100\n",
            "110/110 [==============================] - 58s 526ms/step - loss: 0.4581 - val_loss: 0.6366\n",
            "Epoch 38/100\n",
            "110/110 [==============================] - 61s 554ms/step - loss: 0.4508 - val_loss: 0.6472\n",
            "Epoch 39/100\n",
            "110/110 [==============================] - 66s 601ms/step - loss: 0.4466 - val_loss: 0.6420\n",
            "Epoch 40/100\n",
            "110/110 [==============================] - 54s 497ms/step - loss: 0.4445 - val_loss: 0.6396\n",
            "Epoch 41/100\n",
            "110/110 [==============================] - 52s 469ms/step - loss: 0.4382 - val_loss: 0.6488\n",
            "Epoch 42/100\n",
            "110/110 [==============================] - 62s 567ms/step - loss: 0.4368 - val_loss: 0.6309\n",
            "Epoch 43/100\n",
            "110/110 [==============================] - 59s 537ms/step - loss: 0.4304 - val_loss: 0.6302\n",
            "Epoch 44/100\n",
            "110/110 [==============================] - 58s 529ms/step - loss: 0.4277 - val_loss: 0.6408\n",
            "Epoch 45/100\n",
            "110/110 [==============================] - 54s 496ms/step - loss: 0.4270 - val_loss: 0.6331\n",
            "Epoch 46/100\n",
            "110/110 [==============================] - 57s 521ms/step - loss: 0.4198 - val_loss: 0.6258\n",
            "Epoch 47/100\n",
            "110/110 [==============================] - 65s 590ms/step - loss: 0.4161 - val_loss: 0.6328\n",
            "Epoch 48/100\n",
            "110/110 [==============================] - 52s 477ms/step - loss: 0.4137 - val_loss: 0.6292\n",
            "Epoch 49/100\n",
            "110/110 [==============================] - 54s 488ms/step - loss: 0.4107 - val_loss: 0.6229\n",
            "Epoch 50/100\n",
            "110/110 [==============================] - 70s 639ms/step - loss: 0.4060 - val_loss: 0.6221\n",
            "Epoch 51/100\n",
            "110/110 [==============================] - 55s 501ms/step - loss: 0.4054 - val_loss: 0.6201\n",
            "Epoch 52/100\n",
            "110/110 [==============================] - 57s 521ms/step - loss: 0.3996 - val_loss: 0.6312\n",
            "Epoch 53/100\n",
            "110/110 [==============================] - 55s 499ms/step - loss: 0.3998 - val_loss: 0.6258\n",
            "Epoch 54/100\n",
            "110/110 [==============================] - 71s 651ms/step - loss: 0.3943 - val_loss: 0.6138\n",
            "Epoch 55/100\n",
            "110/110 [==============================] - 54s 494ms/step - loss: 0.3907 - val_loss: 0.6176\n",
            "Epoch 56/100\n",
            "110/110 [==============================] - 57s 518ms/step - loss: 0.3877 - val_loss: 0.6119\n",
            "Epoch 57/100\n",
            "110/110 [==============================] - 55s 503ms/step - loss: 0.3880 - val_loss: 0.6169\n",
            "Epoch 58/100\n",
            "110/110 [==============================] - 64s 582ms/step - loss: 0.3838 - val_loss: 0.6182\n",
            "Epoch 59/100\n",
            "110/110 [==============================] - 52s 476ms/step - loss: 0.3783 - val_loss: 0.6202\n",
            "Epoch 60/100\n",
            "110/110 [==============================] - 59s 539ms/step - loss: 0.3770 - val_loss: 0.6146\n",
            "Epoch 61/100\n",
            "110/110 [==============================] - 53s 486ms/step - loss: 0.3730 - val_loss: 0.6273\n",
            "Epoch 62/100\n",
            "110/110 [==============================] - 63s 576ms/step - loss: 0.3722 - val_loss: 0.6114\n",
            "Epoch 63/100\n",
            "110/110 [==============================] - 52s 470ms/step - loss: 0.3695 - val_loss: 0.6131\n",
            "Epoch 64/100\n",
            "110/110 [==============================] - 54s 486ms/step - loss: 0.3665 - val_loss: 0.6104\n",
            "Epoch 65/100\n",
            "110/110 [==============================] - 70s 641ms/step - loss: 0.3631 - val_loss: 0.6106\n",
            "Epoch 66/100\n",
            "110/110 [==============================] - 56s 508ms/step - loss: 0.3621 - val_loss: 0.6105\n",
            "Epoch 67/100\n",
            "110/110 [==============================] - 57s 520ms/step - loss: 0.3583 - val_loss: 0.6262\n",
            "Epoch 68/100\n",
            "110/110 [==============================] - 56s 510ms/step - loss: 0.3705 - val_loss: 0.6114\n",
            "Epoch 69/100\n",
            "110/110 [==============================] - 65s 597ms/step - loss: 0.3715 - val_loss: 0.6101\n",
            "Epoch 70/100\n",
            "110/110 [==============================] - 55s 502ms/step - loss: 0.3552 - val_loss: 0.6140\n",
            "Epoch 71/100\n",
            "110/110 [==============================] - 59s 543ms/step - loss: 0.3504 - val_loss: 0.6109\n",
            "Epoch 72/100\n",
            "110/110 [==============================] - 61s 553ms/step - loss: 0.3499 - val_loss: 0.6175\n",
            "Epoch 73/100\n",
            "110/110 [==============================] - 61s 551ms/step - loss: 0.3505 - val_loss: 0.6268\n",
            "Epoch 74/100\n",
            "110/110 [==============================] - 58s 527ms/step - loss: 0.3464 - val_loss: 0.6161\n",
            "Epoch 75/100\n",
            "110/110 [==============================] - 57s 520ms/step - loss: 0.3409 - val_loss: 0.6101\n",
            "Epoch 76/100\n",
            "110/110 [==============================] - 67s 609ms/step - loss: 0.3422 - val_loss: 0.6151\n",
            "Epoch 77/100\n",
            "110/110 [==============================] - 60s 543ms/step - loss: 0.3402 - val_loss: 0.6194\n",
            "Epoch 78/100\n",
            "110/110 [==============================] - 54s 494ms/step - loss: 0.3375 - val_loss: 0.6226\n",
            "Epoch 79/100\n",
            "110/110 [==============================] - 64s 579ms/step - loss: 0.3353 - val_loss: 0.6088\n",
            "Epoch 80/100\n",
            "110/110 [==============================] - 62s 561ms/step - loss: 0.3445 - val_loss: 0.6248\n",
            "Epoch 81/100\n",
            "110/110 [==============================] - 54s 492ms/step - loss: 0.3394 - val_loss: 0.6271\n",
            "Epoch 82/100\n",
            "110/110 [==============================] - 56s 505ms/step - loss: 0.3302 - val_loss: 0.6083\n",
            "Epoch 83/100\n",
            "110/110 [==============================] - 67s 612ms/step - loss: 0.3361 - val_loss: 0.6156\n",
            "Epoch 84/100\n",
            "110/110 [==============================] - 53s 485ms/step - loss: 0.3301 - val_loss: 0.6180\n",
            "Epoch 85/100\n",
            "110/110 [==============================] - 59s 540ms/step - loss: 0.3273 - val_loss: 0.6195\n",
            "Epoch 86/100\n",
            "110/110 [==============================] - 65s 592ms/step - loss: 0.3281 - val_loss: 0.6128\n",
            "Epoch 87/100\n",
            "110/110 [==============================] - 56s 508ms/step - loss: 0.3226 - val_loss: 0.6121\n",
            "Epoch 88/100\n",
            "110/110 [==============================] - 58s 533ms/step - loss: 0.3227 - val_loss: 0.6153\n",
            "Epoch 89/100\n",
            "110/110 [==============================] - 54s 487ms/step - loss: 0.3227 - val_loss: 0.6168\n",
            "Epoch 90/100\n",
            "110/110 [==============================] - 66s 601ms/step - loss: 0.3193 - val_loss: 0.6162\n",
            "Epoch 91/100\n",
            "110/110 [==============================] - 58s 530ms/step - loss: 0.3164 - val_loss: 0.6137\n",
            "Epoch 92/100\n",
            "110/110 [==============================] - 55s 500ms/step - loss: 0.3238 - val_loss: 0.6220\n",
            "Epoch 93/100\n",
            "110/110 [==============================] - 64s 584ms/step - loss: 0.3435 - val_loss: 0.6118\n",
            "Epoch 94/100\n",
            "110/110 [==============================] - 56s 504ms/step - loss: 0.3215 - val_loss: 0.6145\n",
            "Epoch 95/100\n",
            "110/110 [==============================] - 54s 489ms/step - loss: 0.3190 - val_loss: 0.6179\n",
            "Epoch 96/100\n",
            "110/110 [==============================] - 60s 547ms/step - loss: 0.3099 - val_loss: 0.6253\n",
            "Epoch 97/100\n",
            "110/110 [==============================] - 65s 597ms/step - loss: 0.3118 - val_loss: 0.6176\n",
            "Epoch 98/100\n",
            "110/110 [==============================] - 55s 504ms/step - loss: 0.3159 - val_loss: 0.6203\n",
            "Epoch 99/100\n",
            "110/110 [==============================] - 59s 541ms/step - loss: 0.3350 - val_loss: 0.6194\n",
            "Epoch 100/100\n",
            "110/110 [==============================] - 63s 575ms/step - loss: 0.3276 - val_loss: 0.6264\n"
          ]
        }
      ],
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=WEIGHT_FILE_PATH, save_best_only=True)\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
        "          verbose=1, validation_split=0.3, callbacks=[checkpoint])\n",
        "\n",
        "model.save_weights(WEIGHT_FILE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vvi-dIXAEsZ8",
        "outputId": "d26b5003-77ed-451f-c73e-e36fdc06aa84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_25\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer  [(None, None, 100)]          0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer  [(None, None, 116)]          0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " encoder_lstm (LSTM)         [(None, 256),                365568    ['encoder_inputs[0][0]']      \n",
            "                              (None, 256),                                                        \n",
            "                              (None, 256)]                                                        \n",
            "                                                                                                  \n",
            " decoder_lstm (LSTM)         [(None, None, 256),          381952    ['decoder_inputs[0][0]',      \n",
            "                              (None, 256),                           'encoder_lstm[0][1]',        \n",
            "                              (None, 256)]                           'encoder_lstm[0][2]']        \n",
            "                                                                                                  \n",
            " decoder_dense (Dense)       (None, None, 116)            29812     ['decoder_lstm[0][0]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 777332 (2.97 MB)\n",
            "Trainable params: 777332 (2.97 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "KB0DUwZXEsZ9"
      },
      "outputs": [],
      "source": [
        "encoder_model_inf = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "\n",
        "decoder_state_input_h = Input(shape=(HIDDEN_UNITS,))\n",
        "decoder_state_input_c = Input(shape=(HIDDEN_UNITS,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_lstm(decoder_inputs,\n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_inputs] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "ybBqcoo4EsZ9"
      },
      "outputs": [],
      "source": [
        "max_encoder_seq_length = context['encoder_max_seq_length']\n",
        "max_decoder_seq_length = context['decoder_max_seq_length']\n",
        "num_decoder_tokens = context['num_decoder_tokens']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "R_44M9cfEsZ-"
      },
      "outputs": [],
      "source": [
        "def predict_sent(input_text):\n",
        "        input_seq = []\n",
        "        input_wids = []\n",
        "        for word in nltk.word_tokenize(input_text.lower()):\n",
        "            emb = unknown_emb\n",
        "            if word in word2em:\n",
        "                emb = word2em[word]\n",
        "            input_wids.append(emb)\n",
        "        input_seq.append(input_wids)\n",
        "        input_seq = pad_sequences(input_seq, max_encoder_seq_length)\n",
        "        states_value = encoder_model_inf.predict(input_seq)\n",
        "        target_seq = np.zeros((1, 1,num_decoder_tokens))\n",
        "        target_seq[0, 0, target_word2idx['\\t']] = 1\n",
        "        target_text = ''\n",
        "        terminated = False\n",
        "        while not terminated:\n",
        "            output_tokens, h, c = decoder_model_inf.predict([target_seq] + states_value)\n",
        "\n",
        "            sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
        "            sample_word = target_idx2word[sample_token_idx]\n",
        "            target_text += sample_word\n",
        "\n",
        "            if sample_word == '\\n' or len(target_text) >= max_decoder_seq_length:\n",
        "                terminated = True\n",
        "\n",
        "            target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "            target_seq[0, 0, sample_token_idx] = 1\n",
        "\n",
        "            states_value = [h, c]\n",
        "        return target_text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAGMm3ytEsZ-",
        "outputId": "f7088566-5f31-48e3-be74-78e20f42cd1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 4s 4s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "C'est grant.\n"
          ]
        }
      ],
      "source": [
        "print(predict_sent('this is amazing.'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "_-IBxVW4EsZ-"
      },
      "outputs": [],
      "source": [
        "def calculate_bleu_score(reference, candidate):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    return sentence_bleu([reference], candidate, smoothing_function=smoothie)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = 'This is amazing.'\n",
        "predicted_sentence = predict_sent(input_sentence)\n",
        "print('Predicted:', predicted_sentence)\n",
        "\n",
        "reference_sentence = 'Ceci est incroyable.'  # Reference translation\n",
        "bleu_score = calculate_bleu_score(reference_sentence.split(), predicted_sentence.split())\n",
        "print('BLEU score:', bleu_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsbPKlftmNp4",
        "outputId": "e419b8cd-8419-4189-9a57-d84685b5b62c"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "Predicted: C'est grant.\n",
            "BLEU score: 0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:tfp3.6]",
      "language": "python",
      "name": "conda-env-tfp3.6-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}